{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b488e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta absoluta a src para cualquier modo de ejecución\n",
    "import sys\n",
    "src_path = r'C:\\Users\\rijar\\Proyectos\\Tesis\\estructura_final\\src'\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2bc19",
   "metadata": {},
   "source": [
    "# Importar librerías y funciones\n",
    "import sys, os\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "import joblib\n",
    "import evaluation\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos preprocesados\n",
    "X1_train, y1_train, X1_test, y1_test = joblib.load('../data/dataset1_processed.joblib')\n",
    "X2_train, y2_train, X2_test, y2_test = joblib.load('../data/dataset2_processed.joblib')\n",
    "X3_train, y3_train, X3_test, y3_test = joblib.load('../data/dataset3_processed.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bff898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos preprocesados para los tres datasets\n",
    "import joblib\n",
    "X1_train, y1_train, X1_test, y1_test = joblib.load('../data/dataset1_processed.joblib')\n",
    "X2_train, y2_train, X2_test, y2_test = joblib.load('../data/dataset2_processed.joblib')\n",
    "X3_train, y3_train, X3_test, y3_test = joblib.load('../data/dataset3_processed.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b600101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelos para dataset1\n",
      "  LR: {'accuracy': 0.875, 'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'auc': 1.0}\n",
      "  NB: {'accuracy': 0.875, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5714285714285714}\n",
      "  KNN: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'auc': 1.0}\n",
      "  SVM: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'auc': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rijar\\Proyectos\\Tesis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\rijar\\Proyectos\\Tesis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DT: {'accuracy': 0.875, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5}\n",
      "  MLP: {'accuracy': 0.875, 'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'auc': 1.0}\n",
      "  stacking_model: {'accuracy': 0.875, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 1.0}\n",
      "  voting_model: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'auc': 1.0}\n",
      "Métricas guardadas para dataset1: KNN\n",
      "Evaluando modelos para dataset2\n",
      "  LR: {'accuracy': 0.5, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.6666666666666667}\n",
      "  NB: {'accuracy': 0.5, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.6666666666666667}\n",
      "  KNN: {'accuracy': 0.5, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rijar\\Proyectos\\Tesis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SVM: {'accuracy': 0.375, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.4166666666666667}\n",
      "  DT: {'accuracy': 0.5, 'precision': 0.25, 'recall': 0.5, 'f1': 0.3333333333333333, 'auc': 0.625}\n",
      "  MLP: {'accuracy': 0.75, 'precision': 0.5, 'recall': 0.5, 'f1': 0.5, 'auc': 0.7500000000000001}\n",
      "  stacking_model: {'accuracy': 0.625, 'precision': 0.3333333333333333, 'recall': 0.5, 'f1': 0.4, 'auc': 0.5}\n",
      "  voting_model: {'accuracy': 0.625, 'precision': 0.3333333333333333, 'recall': 0.5, 'f1': 0.4, 'auc': 0.5}\n",
      "Métricas guardadas para dataset2: MLP\n",
      "Evaluando modelos para dataset3\n",
      "  LR: {'accuracy': 0.875, 'precision': 0.875, 'recall': 1.0, 'f1': 0.9333333333333333, 'auc': 1.0}\n",
      "  NB: {'accuracy': 0.75, 'precision': 1.0, 'recall': 0.7142857142857143, 'f1': 0.8333333333333334, 'auc': 1.0}\n",
      "  KNN: {'accuracy': 0.875, 'precision': 0.875, 'recall': 1.0, 'f1': 0.9333333333333333, 'auc': 0.9285714285714286}\n",
      "  SVM: {'accuracy': 0.875, 'precision': 0.875, 'recall': 1.0, 'f1': 0.9333333333333333, 'auc': 0.5714285714285714}\n",
      "  DT: {'accuracy': 0.625, 'precision': 0.8333333333333334, 'recall': 0.7142857142857143, 'f1': 0.7692307692307693, 'auc': 0.35714285714285715}\n",
      "  MLP: {'accuracy': 0.75, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1': 0.8571428571428571, 'auc': 0.7142857142857143}\n",
      "  stacking_model: {'accuracy': 0.875, 'precision': 0.875, 'recall': 1.0, 'f1': 0.9333333333333333, 'auc': 0.7142857142857143}\n",
      "  voting_model: {'accuracy': 0.75, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1': 0.8571428571428571, 'auc': 0.8571428571428571}\n",
      "Métricas guardadas para dataset3: LR\n"
     ]
    }
   ],
   "source": [
    "# Evaluar modelos y guardar métricas para la app web (todo en una celda)\n",
    "import sys, os\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "import joblib\n",
    "import evaluation\n",
    "model_types = ['LR', 'NB', 'KNN', 'SVM', 'DT', 'MLP', 'stacking_model', 'voting_model']\n",
    "results = {}\n",
    "for ds_name, (X_test, y_test) in zip(\n",
    "    ['dataset1', 'dataset2', 'dataset3'],\n",
    "    [(X1_test, y1_test), (X2_test, y2_test), (X3_test, y3_test)]\n",
    "):\n",
    "    print(f'Evaluando modelos para {ds_name}')\n",
    "    ds_results = {}\n",
    "    for mtype in model_types:\n",
    "        try:\n",
    "            if mtype in ['stacking_model', 'voting_model']:\n",
    "                model = joblib.load(f'../data/{ds_name}_{mtype}.joblib')\n",
    "            else:\n",
    "                model = joblib.load(f'../data/{ds_name}_{mtype}_model.joblib')\n",
    "            metrics = evaluation.evaluate_model(model, X_test, y_test)\n",
    "            ds_results[mtype] = metrics\n",
    "            print(f'  {mtype}:', metrics)\n",
    "        except Exception as e:\n",
    "            print(f'  {mtype}: No disponible. Error: {e}')\n",
    "    results[ds_name] = ds_results\n",
    "    # Guardar métricas del mejor modelo individual (el que tenga mayor accuracy)\n",
    "    best_model = None\n",
    "    best_acc = -1\n",
    "    best_metrics = None\n",
    "    for mtype, metrics in ds_results.items():\n",
    "        if 'accuracy' in metrics and metrics['accuracy'] > best_acc:\n",
    "            best_acc = metrics['accuracy']\n",
    "            best_model = mtype\n",
    "            best_metrics = metrics\n",
    "    if best_metrics is not None:\n",
    "        joblib.dump(best_metrics, f\"../data/{ds_name}_metrics.joblib\")\n",
    "        print(f\"Métricas guardadas para {ds_name}: {best_model}\")\n",
    "    else:\n",
    "        print(f\"No se encontraron métricas para {ds_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
